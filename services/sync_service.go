package services

import (
	"context"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"Off-chainDatainDexer/blockchain"
	"Off-chainDatainDexer/config"
	"Off-chainDatainDexer/database"
	"Off-chainDatainDexer/utils"

	"gorm.io/gorm"
)

// SyncService æ•°æ®åŒæ­¥æœåŠ¡
type SyncService struct {
	db           *gorm.DB
	mu           sync.RWMutex
	status       SyncStatus
	stopCh       chan struct{}
	tronClient   *blockchain.TronHTTPClient
	config       *config.Config
	tronUtils    *utils.TronUtils
	isRunning    bool
	syncInterval time.Duration
	// æ–°å¢å­—æ®µç”¨äºå¤±è´¥åŒºå—é‡è¯•
	failedBlocks map[uint64]*FailedBlock
	failedMutex  sync.RWMutex
	// æ–°å¢å¹¶å‘åŒæ­¥ç®¡ç†å™¨
	concurrentManager *ConcurrentSyncManager
	concurrentEnabled bool
	// åŒæ­¥è¿›åº¦åè°ƒå’Œæ•°æ®ä¸€è‡´æ€§
	syncProgress    *SyncProgress
	progressMu      sync.RWMutex
	processedBlocks map[uint64]bool // å·²å¤„ç†åŒºå—è®°å½•
	processedMu     sync.RWMutex
	// æ–°å¢æœåŠ¡ä¾èµ–ç”¨äºç¼“å­˜æ›´æ–°
	transferService *TransferService
	marketService   *MarketService
	// è¿›åº¦æ¡æ˜¾ç¤ºæ§åˆ¶
	lastProgressDisplay time.Time
	progressDisplayInterval time.Duration
	
	// å¥åº·æ£€æŸ¥çŠ¶æ€è·Ÿè¸ª
	lastHealthStatus string
	lastAlertLevel   string
}

// FailedBlock å¤±è´¥åŒºå—ä¿¡æ¯
type FailedBlock struct {
	BlockNumber uint64    `json:"block_number"`
	FailCount   int       `json:"fail_count"`
	LastAttempt time.Time `json:"last_attempt"`
	LastError   string    `json:"last_error"`
}

// SyncTask åŒæ­¥ä»»åŠ¡
type SyncTask struct {
	StartBlock uint64
	EndBlock   uint64
	NodeIndex  int // æŒ‡å®šä½¿ç”¨çš„èŠ‚ç‚¹ç´¢å¼•
	RetryCount int
	CreatedAt  time.Time
}

// SyncResult åŒæ­¥ç»“æœ
type SyncResult struct {
	Task      *SyncTask
	Transfers []*blockchain.TransferEvent
	Error     error
	Duration  time.Duration
}

// ConcurrentSyncManager å¹¶å‘åŒæ­¥ç®¡ç†å™¨
type ConcurrentSyncManager struct {
	taskChan    chan *SyncTask
	resultChan  chan *SyncResult
	workerCount int
	tronClient  *blockchain.TronHTTPClient
	ctx         context.Context
	cancel      context.CancelFunc
	wg          sync.WaitGroup
}

// SyncProgress åŒæ­¥è¿›åº¦è·Ÿè¸ª
type SyncProgress struct {
	CurrentBlock    uint64        `json:"current_block"`
	TargetBlock     uint64        `json:"target_block"`
	ProcessedBlocks uint64        `json:"processed_blocks"`
	FailedBlocks    uint64        `json:"failed_blocks"`
	StartTime       time.Time     `json:"start_time"`
	LastUpdateTime  time.Time     `json:"last_update_time"`
	SyncRate        float64       `json:"sync_rate"`      // åŒºå—/ç§’
	EstimatedTime   time.Duration `json:"estimated_time"` // é¢„è®¡å®Œæˆæ—¶é—´
	ActiveWorkers   int           `json:"active_workers"`
	TotalTasks      int           `json:"total_tasks"`
	CompletedTasks  int           `json:"completed_tasks"`
}

// SyncConfig åŒæ­¥é…ç½®
type SyncConfig struct {
	Interval   time.Duration `json:"interval"`
	BatchSize  int           `json:"batch_size"`
	StartBlock uint64        `json:"start_block"`
	AutoStart  bool          `json:"auto_start"`
}

// SyncStatus åŒæ­¥çŠ¶æ€
type SyncStatus struct {
	IsRunning    bool          `json:"is_running"`
	LastSyncTime time.Time     `json:"last_sync_time"`
	SyncInterval time.Duration `json:"sync_interval"`
	LastBlock    uint64        `json:"last_block"`
	CurrentBlock uint64        `json:"current_block"`
	LatestBlock  uint64        `json:"latest_block"`
	SyncedCount  int64         `json:"synced_count"`
	ErrorCount   int64         `json:"error_count"`
	LastError    string        `json:"last_error"`
}

// NewSyncService åˆ›å»ºæ–°çš„åŒæ­¥æœåŠ¡
func NewSyncService(db *gorm.DB, transferService *TransferService, marketService *MarketService) (*SyncService, error) {
	// åŠ è½½é…ç½®
	cfg := config.LoadConfig()

	// è·å–åŒæ­¥é—´éš”
	syncInterval := cfg.SyncInterval

	// è·å–èµ·å§‹åŒºå—å·
	startBlock := cfg.StartBlock

	// åˆ›å»ºæ³¢åœºHTTPå®¢æˆ·ç«¯ï¼ˆæ”¯æŒå¤šèŠ‚ç‚¹é…ç½®ï¼‰
	var tronClient *blockchain.TronHTTPClient
	if len(cfg.TronNodes) > 0 {
		// ä½¿ç”¨å¤šèŠ‚ç‚¹é…ç½®
		tronClient = blockchain.NewTronHTTPClientWithConfig(cfg)
	} else {
		// ä½¿ç”¨å•èŠ‚ç‚¹é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
		tronClient = blockchain.NewTronHTTPClient(cfg.TronNodeURL, cfg.TronAPIKey, cfg.USDTContract)
	}

	// åˆ›å»ºæ³¢åœºå·¥å…·
	tronUtils := utils.NewTronUtils()

	// æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶å‘åŒæ­¥ï¼ˆå½“é…ç½®äº†å¤šä¸ªèŠ‚ç‚¹ä¸”å·¥ä½œåç¨‹æ•°å¤§äº1æ—¶ï¼‰
	concurrentEnabled := len(cfg.TronNodes) > 1 && cfg.ConcurrentConfig.WorkerCount > 1

	service := &SyncService{
		db: db,
		status: SyncStatus{
			IsRunning:    false,
			LastSyncTime: time.Time{},
			SyncInterval: syncInterval,
			LastBlock:    startBlock,
			CurrentBlock: 0,
			LatestBlock:  0,
			SyncedCount:  0,
			ErrorCount:   0,
			LastError:    "",
		},
		stopCh:       make(chan struct{}),
		tronClient:   tronClient,
		config:       cfg,
		tronUtils:    tronUtils,
		isRunning:    false,
		syncInterval: syncInterval,
		// åˆå§‹åŒ–å¤±è´¥åŒºå—æ˜ å°„
		failedBlocks:      make(map[uint64]*FailedBlock),
		concurrentEnabled: concurrentEnabled,
		// åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªç›¸å…³å­—æ®µ
		processedBlocks: make(map[uint64]bool),
		syncProgress: &SyncProgress{
			StartTime:      time.Now(),
			LastUpdateTime: time.Now(),
		},
		// åˆå§‹åŒ–æœåŠ¡ä¾èµ–
		transferService: transferService,
		marketService:   marketService,
		// åˆå§‹åŒ–è¿›åº¦æ¡æ˜¾ç¤ºæ§åˆ¶
		lastProgressDisplay:     time.Time{},
		progressDisplayInterval: 5 * time.Second, // æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡
		// åˆå§‹åŒ–å¥åº·æ£€æŸ¥çŠ¶æ€è·Ÿè¸ª
		lastHealthStatus: "",
		lastAlertLevel:   "",
	}

	// å¦‚æœå¯ç”¨å¹¶å‘åŒæ­¥ï¼Œåˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨
	if concurrentEnabled {
		service.concurrentManager = service.newConcurrentSyncManager()
		utils.Info("Concurrent sync enabled with %d workers", cfg.ConcurrentConfig.WorkerCount)
	} else {
		utils.Info("Using sequential sync mode")
	}

	return service, nil
}

// StartSync å¼€å§‹æ•°æ®åŒæ­¥
func (s *SyncService) StartSync() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.status.IsRunning {
		return fmt.Errorf("sync service is already running")
	}

	s.status.IsRunning = true
	s.stopCh = make(chan struct{})

	utils.FileLog(utils.INFO, "Starting Tron USDT sync service...")

	// æ˜¾ç¤ºèŠ‚ç‚¹é…ç½®ä¿¡æ¯
	if len(s.config.TronNodes) > 0 {
		utils.FileLog(utils.INFO, "=== å¤šèŠ‚ç‚¹é…ç½® ===")
		utils.FileLog(utils.INFO, "æ€»èŠ‚ç‚¹æ•°: %d", len(s.config.TronNodes))
		for i, node := range s.config.TronNodes {
			apiKeyStatus := "æ— API Key"
			if node.APIKey != "" {
				apiKeyStatus = "æœ‰API Key"
			}
			utils.FileLog(utils.INFO, "èŠ‚ç‚¹ %d: %s (æƒé‡: %d, %s)", i, node.URL, node.Weight, apiKeyStatus)
		}
		utils.FileLog(utils.INFO, "================")
	} else {
		// å‘åå…¼å®¹ï¼šæ˜¾ç¤ºå•èŠ‚ç‚¹é…ç½®
		utils.FileLog(utils.INFO, "Node URL: %s", s.config.TronNodeURL)
		apiKeyStatus := "æ— API Key"
		if s.config.TronAPIKey != "" {
			apiKeyStatus = "æœ‰API Key"
		}
		utils.FileLog(utils.INFO, "API KeyçŠ¶æ€: %s", apiKeyStatus)
	}

	utils.FileLog(utils.INFO, "USDT Contract: %s", s.config.USDTContract)
	utils.FileLog(utils.INFO, "Sync Interval: %v", s.status.SyncInterval)
	utils.FileLog(utils.INFO, "Start Block: %d", s.status.LastBlock)

	// å¦‚æœå¯ç”¨å¹¶å‘åŒæ­¥ï¼Œå¯åŠ¨å¹¶å‘ç®¡ç†å™¨
	if s.concurrentEnabled {
		s.startConcurrentSync()
		utils.FileLog(utils.INFO, "Concurrent sync started with %d workers", s.config.ConcurrentConfig.WorkerCount)
	}

	// å¯åŠ¨åŒæ­¥å¾ªç¯
	go s.syncLoop()

	return nil
}

// StopSync åœæ­¢æ•°æ®åŒæ­¥
func (s *SyncService) StopSync() {
	s.mu.Lock()
	defer s.mu.Unlock()

	if !s.status.IsRunning {
		return
	}

	utils.FileLog(utils.INFO, "Stopping Tron USDT sync service...")
	s.status.IsRunning = false

	// å¦‚æœå¯ç”¨å¹¶å‘åŒæ­¥ï¼Œåœæ­¢å¹¶å‘ç®¡ç†å™¨
	if s.concurrentEnabled {
		s.stopConcurrentSync()
		utils.FileLog(utils.INFO, "Concurrent sync stopped")
	}

	close(s.stopCh)
	utils.FileLog(utils.INFO, "Sync service stopped")
}

// GetSyncStatus è·å–åŒæ­¥çŠ¶æ€
func (s *SyncService) GetSyncStatus() SyncStatus {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.status
}

// syncLoop åŒæ­¥å¾ªç¯
func (s *SyncService) syncLoop() {
	ticker := time.NewTicker(s.syncInterval)
	defer ticker.Stop()

	for {
		select {
		case <-s.stopCh:
			log.Println("Sync loop stopped")
			return
		case <-ticker.C:
			if err := s.performSync(); err != nil {
				log.Printf("Sync error: %v", err)
				s.mu.Lock()
				s.status.ErrorCount++
				s.status.LastError = err.Error()
				s.mu.Unlock()
			} else {
				s.mu.Lock()
				s.status.LastSyncTime = time.Now()
				s.mu.Unlock()
			}
		}
	}
}

// updateSyncProgress æ›´æ–°åŒæ­¥è¿›åº¦
func (s *SyncService) updateSyncProgress(currentBlock, targetBlock uint64) {
	s.progressMu.Lock()
	defer s.progressMu.Unlock()

	now := time.Now()
	oldCurrentBlock := s.syncProgress.CurrentBlock
	s.syncProgress.CurrentBlock = currentBlock
	s.syncProgress.TargetBlock = targetBlock
	s.syncProgress.LastUpdateTime = now

	// å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œè®°å½•èµ·å§‹æ—¶é—´å’Œèµ·å§‹åŒºå—
	if s.syncProgress.StartTime.IsZero() {
		s.syncProgress.StartTime = now
	}

	// è®¡ç®—åŒæ­¥èµ·å§‹åŒºå—ï¼ˆä½¿ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„CurrentBlockï¼‰
	startBlock := oldCurrentBlock
	if startBlock == 0 {
		startBlock = currentBlock // å¦‚æœæ²¡æœ‰åˆå§‹å€¼ï¼Œä½¿ç”¨å½“å‰åŒºå—
	}

	// è®¡ç®—å·²å¤„ç†çš„åŒºå—æ•°é‡ï¼ˆç›¸å¯¹äºåŒæ­¥èŒƒå›´ï¼‰
	if currentBlock >= startBlock {
		s.syncProgress.ProcessedBlocks = currentBlock - startBlock + 1
	} else {
		s.syncProgress.ProcessedBlocks = 1 // è‡³å°‘å¤„ç†äº†1ä¸ªåŒºå—
	}

	// è®¡ç®—åŒæ­¥é€Ÿç‡
	elapsed := now.Sub(s.syncProgress.StartTime).Seconds()
	if elapsed > 0 && s.syncProgress.ProcessedBlocks > 0 {
		s.syncProgress.SyncRate = float64(s.syncProgress.ProcessedBlocks) / elapsed
	}

	// ä¼°ç®—å‰©ä½™æ—¶é—´
	remainingBlocks := targetBlock - currentBlock
	if s.syncProgress.SyncRate > 0 && remainingBlocks > 0 {
		s.syncProgress.EstimatedTime = time.Duration(float64(remainingBlocks)/s.syncProgress.SyncRate) * time.Second
	} else {
		s.syncProgress.EstimatedTime = 0
	}

	// æ˜¾ç¤ºå¯è§†åŒ–è¿›åº¦æ¡
	s.displayProgressBar(currentBlock, targetBlock)
}

// displayProgressBar æ˜¾ç¤ºå¯è§†åŒ–è¿›åº¦æ¡
func (s *SyncService) displayProgressBar(currentBlock, targetBlock uint64) {
	if targetBlock == 0 {
		return
	}

	// æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆæ—¶é—´é—´éš”æ§åˆ¶ï¼‰
	now := time.Now()
	if !s.lastProgressDisplay.IsZero() && now.Sub(s.lastProgressDisplay) < s.progressDisplayInterval {
		return // è¿˜æœªåˆ°æ˜¾ç¤ºæ—¶é—´ï¼Œè·³è¿‡
	}
	s.lastProgressDisplay = now

	// è·å–åŒæ­¥è¿›åº¦ä¿¡æ¯
	s.progressMu.RLock()
	startBlock := s.syncProgress.CurrentBlock
	if startBlock == 0 {
		startBlock = currentBlock
	}
	totalBlocks := targetBlock - startBlock + 1
	processedBlocks := s.syncProgress.ProcessedBlocks
	s.progressMu.RUnlock()

	// è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆåŸºäºå·²å¤„ç†çš„åŒºå—æ•°é‡ï¼‰
	var progress float64
	if totalBlocks > 0 {
		progress = float64(processedBlocks) / float64(totalBlocks)
	}
	if progress > 1.0 {
		progress = 1.0
	}
	percentage := progress * 100

	// è¿›åº¦æ¡é•¿åº¦
	barLength := 50
	filledLength := int(progress * float64(barLength))

	// æ„å»ºè¿›åº¦æ¡
	bar := "["
	for i := 0; i < barLength; i++ {
		if i < filledLength {
			bar += "â–ˆ"
		} else {
			bar += "â–‘"
		}
	}
	bar += "]"

	// æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
	elapsedTime := time.Since(s.syncProgress.StartTime)
	etaStr := "è®¡ç®—ä¸­..."
	if s.syncProgress.EstimatedTime > 0 {
		etaStr = s.formatDuration(s.syncProgress.EstimatedTime)
	}

	// æ ¼å¼åŒ–åŒæ­¥é€Ÿç‡
	rateStr := fmt.Sprintf("%.2f", s.syncProgress.SyncRate)
	if s.syncProgress.SyncRate == 0 {
		rateStr = "è®¡ç®—ä¸­"
	}

	// æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯åˆ°æ§åˆ¶å°
	utils.ProgressBar("\n" +
		"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n" +
		"â•‘                        åŒæ­¥è¿›åº¦                              â•‘\n" +
		"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n" +
		"â•‘ è¿›åº¦: %s %.1f%%                    â•‘\n" +
		"â•‘ åŒºå—: %d / %d                                    â•‘\n" +
		"â•‘ é€Ÿç‡: %s åŒºå—/ç§’                                     â•‘\n" +
		"â•‘ å·²ç”¨æ—¶é—´: %s                                        â•‘\n" +
		"â•‘ é¢„è®¡å‰©ä½™: %s                                        â•‘\n" +
		"â•‘ æ´»è·ƒå·¥ä½œåç¨‹: %d                                         â•‘\n" +
		"â•‘ ä»»åŠ¡è¿›åº¦: %d/%d                                         â•‘\n" +
		"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
		bar, percentage,
		currentBlock, targetBlock,
		rateStr,
		s.formatDuration(elapsedTime),
		etaStr,
		s.syncProgress.ActiveWorkers,
		s.syncProgress.CompletedTasks, s.syncProgress.TotalTasks)
}

// formatDuration æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
func (s *SyncService) formatDuration(d time.Duration) string {
	if d < 0 {
		return "æœªçŸ¥"
	}

	hours := int(d.Hours())
	minutes := int(d.Minutes()) % 60
	seconds := int(d.Seconds()) % 60

	if hours > 0 {
		return fmt.Sprintf("%då°æ—¶%dåˆ†%dç§’", hours, minutes, seconds)
	} else if minutes > 0 {
		return fmt.Sprintf("%dåˆ†%dç§’", minutes, seconds)
	} else {
		return fmt.Sprintf("%dç§’", seconds)
	}
}

// markBlockProcessed æ ‡è®°åŒºå—å·²å¤„ç†
func (s *SyncService) markBlockProcessed(blockNum uint64) {
	s.processedMu.Lock()
	defer s.processedMu.Unlock()

	if !s.processedBlocks[blockNum] {
		s.processedBlocks[blockNum] = true
		s.progressMu.Lock()
		s.syncProgress.ProcessedBlocks++
		s.progressMu.Unlock()
	}
}

// isBlockProcessed æ£€æŸ¥åŒºå—æ˜¯å¦å·²å¤„ç†
func (s *SyncService) isBlockProcessed(blockNum uint64) bool {
	s.processedMu.RLock()
	defer s.processedMu.RUnlock()
	return s.processedBlocks[blockNum]
}

// getSyncProgress è·å–åŒå‰è¿›åº¦
func (s *SyncService) getSyncProgress() *SyncProgress {
	s.progressMu.RLock()
	defer s.progressMu.RUnlock()

	// è¿”å›è¿›åº¦å‰¯æœ¬
	progress := *s.syncProgress
	return &progress
}

// ensureDataConsistency ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
func (s *SyncService) ensureDataConsistency(startBlock, endBlock uint64) error {
	utils.FileLog(utils.INFO, "Checking data consistency for blocks %d-%d", startBlock, endBlock)

	// æ£€æŸ¥æ•°æ®åº“ä¸­çš„åŒºå—è¿ç»­æ€§
	missingBlocks, err := s.findMissingBlocks(startBlock, endBlock)
	if err != nil {
		return fmt.Errorf("failed to check missing blocks: %w", err)
	}

	if len(missingBlocks) > 0 {
		utils.Warn("Found %d missing blocks, adding to retry queue", len(missingBlocks))
		for _, blockNum := range missingBlocks {
			s.addFailedBlock(blockNum, fmt.Errorf("missing block data"))
		}
	}

	// æ£€æŸ¥é‡å¤æ•°æ®
	duplicateCount, err := s.removeDuplicateTransfers(startBlock, endBlock)
	if err != nil {
		return fmt.Errorf("failed to remove duplicates: %w", err)
	}

	if duplicateCount > 0 {
		utils.FileLog(utils.INFO, "Removed %d duplicate transfers for blocks %d-%d", duplicateCount, startBlock, endBlock)
	}

	return nil
}

// findMissingBlocks æŸ¥æ‰¾ç¼ºå¤±çš„åŒºå—
func (s *SyncService) findMissingBlocks(startBlock, endBlock uint64) ([]uint64, error) {
	var existingBlockNums []uint64
	err := s.db.Table("usdt_transfers").
		Select("DISTINCT block_number").
		Where("block_number BETWEEN ? AND ?", startBlock, endBlock).
		Order("block_number").
		Pluck("block_number", &existingBlockNums).Error
	if err != nil {
		return nil, err
	}

	existingBlocks := make(map[uint64]bool)
	for _, blockNum := range existingBlockNums {
		existingBlocks[blockNum] = true
	}

	var missingBlocks []uint64
	for blockNum := startBlock; blockNum <= endBlock; blockNum++ {
		if !existingBlocks[blockNum] && !s.isBlockProcessed(blockNum) {
			missingBlocks = append(missingBlocks, blockNum)
		}
	}

	return missingBlocks, nil
}

// removeDuplicateTransfers ç§»é™¤é‡å¤çš„è½¬è´¦è®°å½•
func (s *SyncService) removeDuplicateTransfers(startBlock, endBlock uint64) (int, error) {
	query := `
		DELETE FROM usdt_transfers 
		WHERE id IN (
			SELECT id FROM (
				SELECT id, ROW_NUMBER() OVER (
					PARTITION BY transaction_hash, from_address, to_address, amount, block_number 
					ORDER BY created_at
				) as rn
				FROM usdt_transfers 
				WHERE block_number BETWEEN ? AND ?
			) t WHERE rn > 1
		)
	`

	result := s.db.Exec(query, startBlock, endBlock)
	if result.Error != nil {
		return 0, result.Error
	}

	return int(result.RowsAffected), nil
}

// performSync æ‰§è¡ŒåŒæ­¥æ“ä½œ
func (s *SyncService) performSync() error {
	utils.FileLog(utils.INFO, "Starting sync operation...")

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer cancel()

	// é¦–å…ˆå¤„ç†å¤±è´¥çš„åŒºå—é‡è¯•
	err := s.retryFailedBlocks(ctx)
	if err != nil {
		utils.FileLog(utils.ERROR, "Failed to retry failed blocks: %v", err)
	}

	// è·å–æœ€æ–°åŒºå—å·
	latestBlock, err := s.tronClient.GetLatestBlockNumber(ctx)
	if err != nil {
		s.updateErrorCount()
		return utils.NewInternalError("Failed to get latest block number", err)
	}

	// ç¡®å®šåŒæ­¥èµ·å§‹åŒºå—
	startBlock := s.status.LastBlock
	if startBlock == 0 {
		// å¦‚æœæ˜¯é¦–æ¬¡åŒæ­¥ï¼Œä»é…ç½®çš„èµ·å§‹åŒºå—å¼€å§‹
		startBlock = s.config.StartBlock
	}

	// å¦‚æœæ²¡æœ‰æ–°åŒºå—ï¼Œè·³è¿‡åŒæ­¥
	if startBlock >= latestBlock {
		utils.FileLog(utils.INFO, "No new blocks to sync")
		return nil
	}

	// é™åˆ¶æ¯æ¬¡åŒæ­¥çš„åŒºå—æ•°é‡
	batchSize := s.config.BatchSize
	if batchSize == 0 {
		batchSize = 100
	}

	endBlock := startBlock + batchSize
	if endBlock > latestBlock {
		endBlock = latestBlock
	}

	utils.FileLog(utils.INFO, "Syncing blocks from %d to %d", startBlock, endBlock)

	// æ›´æ–°å½“å‰åŒæ­¥çŠ¶æ€
	s.mu.Lock()
	s.status.CurrentBlock = endBlock
	s.status.LatestBlock = latestBlock
	s.mu.Unlock()

	// æ ¹æ®æ˜¯å¦å¯ç”¨å¹¶å‘åŒæ­¥é€‰æ‹©ä¸åŒçš„ç­–ç•¥
	if s.concurrentEnabled {
		// å¯åŠ¨å¹¶å‘åŒæ­¥å·¥ä½œåç¨‹
		s.startConcurrentSync()
		// ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶åœæ­¢å¹¶å‘åŒæ­¥
		defer s.stopConcurrentSync()
		return s.performConcurrentSync(ctx, startBlock, endBlock)
	} else {
		return s.performSequentialSync(ctx, startBlock, endBlock)
	}
}

// PerformManualSync æ‰§è¡Œæ‰‹åŠ¨åŒæ­¥æ“ä½œ
func (s *SyncService) PerformManualSync(ctx context.Context) error {
	utils.FileLog(utils.INFO, "Starting manual sync operation...")

	// é¦–å…ˆå¤„ç†å¤±è´¥çš„åŒºå—é‡è¯•
	err := s.retryFailedBlocks(ctx)
	if err != nil {
		utils.Error("Failed to retry failed blocks during manual sync: %v", err)
	}

	// è·å–æœ€æ–°åŒºå—å·
	latestBlock, err := s.tronClient.GetLatestBlockNumber(ctx)
	if err != nil {
		s.updateErrorCount()
		return utils.NewInternalError("Failed to get latest block number", err)
	}

	// ç¡®å®šåŒæ­¥èµ·å§‹åŒºå—
	startBlock := s.status.LastBlock
	if startBlock == 0 {
		// å¦‚æœæ˜¯é¦–æ¬¡åŒæ­¥ï¼Œä»é…ç½®çš„èµ·å§‹åŒºå—å¼€å§‹
		startBlock = s.config.StartBlock
	}

	// å¦‚æœæ²¡æœ‰æ–°åŒºå—ï¼Œè·³è¿‡åŒæ­¥
	if startBlock >= latestBlock {
		utils.Info("No new blocks to sync")
		return nil
	}

	// æ‰‹åŠ¨åŒæ­¥æ—¶ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ä»¥ç¡®ä¿å¿«é€Ÿå“åº”
	batchSize := uint64(50) // æ‰‹åŠ¨åŒæ­¥ä½¿ç”¨å›ºå®šçš„50ä¸ªåŒºå—

	endBlock := startBlock + batchSize
	if endBlock > latestBlock {
		endBlock = latestBlock
	}

	utils.Info("Manual sync: processing blocks from %d to %d", startBlock, endBlock)

	// æ›´æ–°å½“å‰åŒæ­¥çŠ¶æ€
	s.mu.Lock()
	s.status.CurrentBlock = endBlock
	s.status.LatestBlock = latestBlock
	s.mu.Unlock()

	// æ ¹æ®æ˜¯å¦å¯ç”¨å¹¶å‘åŒæ­¥é€‰æ‹©ä¸åŒçš„ç­–ç•¥
	if s.concurrentEnabled {
		return s.performConcurrentSync(ctx, startBlock, endBlock)
	} else {
		return s.performSequentialSync(ctx, startBlock, endBlock)
	}
}

// performSequentialSync æ‰§è¡Œé¡ºåºåŒæ­¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
func (s *SyncService) performSequentialSync(ctx context.Context, startBlock, endBlock uint64) error {
	// è·å–åŒºå—èŒƒå›´å†…çš„USDTè½¬è´¦äº‹ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
	var allTransfers []*blockchain.TransferEvent
	var failedBlocks []uint64

	for blockNum := startBlock; blockNum <= endBlock; blockNum++ {
		transfers, err := s.syncBlockWithRetry(ctx, blockNum)
		if err != nil {
			utils.Error("Failed to sync block %d after retries: %v", blockNum, err)
			// è®°å½•å¤±è´¥çš„åŒºå—
			s.recordFailedBlock(blockNum, err.Error())
			failedBlocks = append(failedBlocks, blockNum)
			continue
		}
		allTransfers = append(allTransfers, transfers...)
	}

	utils.FileLog(utils.INFO, "Found %d USDT transfers in blocks %d-%d", len(allTransfers), startBlock, endBlock)

	// å°†è½¬è´¦æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
	successCount := 0
	for _, transfer := range allTransfers {
		err := s.saveTransferToDatabase(transfer)
		if err != nil {
			utils.FileLog(utils.ERROR, "Failed to save transfer %s: %v", transfer.TxHash, err)
			s.updateErrorCount()
			continue
		}
		successCount++
	}

	// æ›´æ–°åŒæ­¥çŠ¶æ€
	s.mu.Lock()
	// åªæœ‰åœ¨æ²¡æœ‰å¤±è´¥åŒºå—çš„æƒ…å†µä¸‹æ‰æ›´æ–°LastBlock
	if len(failedBlocks) == 0 {
		s.status.LastBlock = endBlock
	} else {
		// å¦‚æœæœ‰å¤±è´¥åŒºå—ï¼Œåªæ›´æ–°åˆ°æœ€åä¸€ä¸ªæˆåŠŸçš„åŒºå—
		lastSuccessBlock := s.findLastSuccessBlock(startBlock, endBlock, failedBlocks)
		s.status.LastBlock = lastSuccessBlock
	}
	s.status.SyncedCount += int64(successCount)
	s.status.LastSyncTime = time.Now()
	s.mu.Unlock()

	utils.FileLog(utils.INFO, "Sequential sync completed. Synced %d transfers, %d failed blocks", successCount, len(failedBlocks))
	return nil
}

// performConcurrentSync æ‰§è¡Œå¹¶å‘åŒæ­¥
func (s *SyncService) performConcurrentSync(ctx context.Context, startBlock, endBlock uint64) error {
	if s.concurrentManager == nil {
		return fmt.Errorf("concurrent manager not initialized")
	}

	// æ›´æ–°åŒæ­¥è¿›åº¦
	s.updateSyncProgress(startBlock, endBlock)

	// è®¡ç®—éœ€è¦åˆ†å‰²çš„ä»»åŠ¡æ•°é‡
	chunkSize := s.config.ConcurrentConfig.ChunkSize
	if chunkSize == 0 {
		chunkSize = 25 // é»˜è®¤æ¯ä¸ªä»»åŠ¡å¤„ç†25ä¸ªåŒºå—
	}

	// åˆ›å»ºåŒæ­¥ä»»åŠ¡
	tasks := s.createSyncTasks(startBlock, endBlock, chunkSize)
	utils.Info("Created %d concurrent sync tasks for blocks %d-%d", len(tasks), startBlock, endBlock)

	// æ›´æ–°è¿›åº¦ä¿¡æ¯
	s.progressMu.Lock()
	s.syncProgress.TotalTasks = len(tasks)
	s.syncProgress.CompletedTasks = 0
	s.syncProgress.ActiveWorkers = s.config.ConcurrentConfig.WorkerCount
	s.progressMu.Unlock()

	// åˆ†å‘ä»»åŠ¡åˆ°å·¥ä½œåç¨‹
	for _, task := range tasks {
		select {
		case s.concurrentManager.taskChan <- task:
		case <-ctx.Done():
			return ctx.Err()
		case <-s.concurrentManager.ctx.Done():
			return fmt.Errorf("concurrent sync manager stopped")
		}
	}

	utils.Info("Concurrent sync tasks dispatched")

	// å¯åŠ¨è¿›åº¦ç›‘æ§
	go s.monitorSyncProgress(startBlock, endBlock)

	// ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
	return s.waitForAllTasksCompletion(ctx, len(tasks))
}

// monitorSyncProgress ç›‘æ§åŒæ­¥è¿›åº¦
func (s *SyncService) monitorSyncProgress(startBlock, endBlock uint64) {
	ticker := time.NewTicker(1 * time.Second) // æ¯1ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡ï¼Œæä¾›å®æ—¶åé¦ˆ
	defer ticker.Stop()

	// åˆå§‹æ˜¾ç¤º
	s.updateSyncProgress(startBlock, endBlock)

	for {
		select {
		case <-ticker.C:
			s.updateSyncProgress(startBlock, endBlock)

			// æ£€æŸ¥æ˜¯å¦å®Œæˆ
			s.progressMu.RLock()
			completed := s.syncProgress.CompletedTasks
			total := s.syncProgress.TotalTasks
			s.progressMu.RUnlock()

			if completed >= total {
				utils.Info("\nğŸ‰ æ‰€æœ‰å¹¶å‘åŒæ­¥ä»»åŠ¡å·²å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆä¸€è‡´æ€§æ£€æŸ¥...")
				if err := s.ensureDataConsistency(startBlock, endBlock); err != nil {
					utils.Error("âŒ æœ€ç»ˆä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: %v", err)
				} else {
					utils.Info("âœ… åŒæ­¥å®Œæˆï¼æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
				}
				return
			}
		case <-s.concurrentManager.ctx.Done():
			return
		}
	}
}

// createSyncTasks åˆ›å»ºåŒæ­¥ä»»åŠ¡åˆ—è¡¨ï¼ˆè€ƒè™‘èŠ‚ç‚¹æƒé‡å’Œè´Ÿè½½å‡è¡¡ï¼‰
func (s *SyncService) createSyncTasks(startBlock, endBlock, chunkSize uint64) []*SyncTask {
	var tasks []*SyncTask
	nodeCount := len(s.config.TronNodes)
	if nodeCount == 0 {
		nodeCount = 1 // å•èŠ‚ç‚¹æ¨¡å¼
	}

	// è®¡ç®—èŠ‚ç‚¹æƒé‡æ€»å’Œ
	totalWeight := 0
	for _, node := range s.config.TronNodes {
		totalWeight += node.Weight
	}
	if totalWeight == 0 {
		totalWeight = nodeCount // å¦‚æœæ²¡æœ‰è®¾ç½®æƒé‡ï¼Œé»˜è®¤æ¯ä¸ªèŠ‚ç‚¹æƒé‡ä¸º1
	}

	// åˆ›å»ºèŠ‚ç‚¹åˆ†é…åºåˆ—ï¼ˆåŸºäºæƒé‡ï¼‰
	nodeSequence := s.createWeightedNodeSequence(totalWeight)
	nodeIndex := 0

	for current := startBlock; current <= endBlock; current += chunkSize {
		taskEnd := current + chunkSize - 1
		if taskEnd > endBlock {
			taskEnd = endBlock
		}

		// æ ¹æ®æƒé‡é€‰æ‹©èŠ‚ç‚¹
		selectedNode := nodeSequence[nodeIndex%len(nodeSequence)]

		task := &SyncTask{
			StartBlock: current,
			EndBlock:   taskEnd,
			NodeIndex:  selectedNode,
			RetryCount: 0,
			CreatedAt:  time.Now(),
		}
		tasks = append(tasks, task)

		// è½®è¯¢åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
		nodeIndex++
	}

	return tasks
}

// createWeightedNodeSequence åˆ›å»ºåŸºäºæƒé‡çš„èŠ‚ç‚¹åˆ†é…åºåˆ—
func (s *SyncService) createWeightedNodeSequence(totalWeight int) []int {
	var sequence []int

	for i, node := range s.config.TronNodes {
		weight := node.Weight
		if weight <= 0 {
			weight = 1 // é»˜è®¤æƒé‡ä¸º1
		}

		// æ ¹æ®æƒé‡æ·»åŠ èŠ‚ç‚¹ç´¢å¼•åˆ°åºåˆ—ä¸­
		for j := 0; j < weight; j++ {
			sequence = append(sequence, i)
		}
	}

	if len(sequence) == 0 {
		// å¦‚æœåºåˆ—ä¸ºç©ºï¼Œåˆ›å»ºç®€å•çš„è½®è¯¢åºåˆ—
		for i := 0; i < len(s.config.TronNodes); i++ {
			sequence = append(sequence, i)
		}
	}

	utils.Info("Created weighted node sequence with %d entries for %d nodes",
		len(sequence), len(s.config.TronNodes))

	return sequence
}

// saveTransferToDatabase å°†è½¬è´¦æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨å¹‚ç­‰æ€§å¤„ç†ï¼‰
func (s *SyncService) saveTransferToDatabase(transfer *blockchain.TransferEvent) error {
	utils.FileLog(utils.INFO, "Saving transfer: %s from %s to %s amount %s",
		transfer.TxHash, transfer.FromAddress, transfer.ToAddress, transfer.Amount.String())

	// å¯¼å…¥databaseåŒ…ä»¥ä½¿ç”¨å¹‚ç­‰æ€§æ–¹æ³•
	// æ³¨æ„ï¼šéœ€è¦åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥
	// "Off-chainDatainDexer/database"

	// åˆ›å»ºæ•°æ®åº“è½¬è´¦è®°å½•
	dbTransfer := &database.Transfer{
		FromAddress:     transfer.FromAddress,
		ToAddress:       transfer.ToAddress,
		Amount:          transfer.Amount.String(),
		TransactionHash: transfer.TxHash,
		BlockNumber:     transfer.BlockNumber,
		Timestamp:       transfer.Timestamp,
	}

	// ä½¿ç”¨å¹‚ç­‰æ€§æ’å…¥æ–¹æ³•
	err := database.InsertTransferIdempotent(dbTransfer)
	if err != nil {
		return utils.NewDatabaseError("Failed to save transfer to database", err)
	}

	utils.FileLog(utils.INFO, "Successfully processed transfer %s", transfer.TxHash)
	return nil
}

// GetLastSyncBlock è·å–æœ€ååŒæ­¥çš„åŒºå—å·
func (s *SyncService) GetLastSyncBlock() uint64 {
	s.mu.RLock()
	defer s.mu.RUnlock()
	return s.status.LastBlock
}

// SetLastSyncBlock è®¾ç½®æœ€ååŒæ­¥çš„åŒºå—å·
func (s *SyncService) SetLastSyncBlock(blockNumber uint64) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.status.LastBlock = blockNumber
}

// GetLatestBlockNumber è·å–åŒºå—é“¾ç½‘ç»œæœ€æ–°åŒºå—å·
func (s *SyncService) GetLatestBlockNumber() (uint64, error) {
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()
	return s.tronClient.GetLatestBlockNumber(ctx)
}

// SyncBlockRange åŒæ­¥æŒ‡å®šåŒºå—èŒƒå›´
func (s *SyncService) SyncBlockRange(ctx context.Context, startBlock, endBlock uint64) error {
	utils.Info("Starting sync for block range %d-%d", startBlock, endBlock)

	// åˆå§‹åŒ–åŒæ­¥è¿›åº¦çŠ¶æ€
	s.progressMu.Lock()
	s.syncProgress.CurrentBlock = startBlock
	s.syncProgress.TargetBlock = endBlock
	s.syncProgress.ProcessedBlocks = 0
	s.syncProgress.FailedBlocks = 0
	s.syncProgress.StartTime = time.Now()
	s.syncProgress.LastUpdateTime = time.Now()
	s.syncProgress.SyncRate = 0
	s.syncProgress.EstimatedTime = 0
	s.syncProgress.ActiveWorkers = 0
	s.syncProgress.TotalTasks = 0
	s.syncProgress.CompletedTasks = 0
	// å­˜å‚¨åŒæ­¥èŒƒå›´ä¿¡æ¯ï¼Œç”¨äºæ­£ç¡®è®¡ç®—è¿›åº¦
	s.syncProgress.CurrentBlock = startBlock
	s.progressMu.Unlock()

	// æ›´æ–°åŒæ­¥è¿›åº¦
	s.updateSyncProgress(startBlock, endBlock)

	// æ ¹æ®æ˜¯å¦å¯ç”¨å¹¶å‘åŒæ­¥é€‰æ‹©ä¸åŒçš„ç­–ç•¥
	if s.concurrentEnabled {
		// å¯åŠ¨å¹¶å‘åŒæ­¥å·¥ä½œåç¨‹
		s.startConcurrentSync()
		// ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶åœæ­¢å¹¶å‘åŒæ­¥
		defer s.stopConcurrentSync()
		return s.performConcurrentSync(ctx, startBlock, endBlock)
	} else {
		return s.performSequentialSync(ctx, startBlock, endBlock)
	}
}

// HealthCheck å¢å¼ºçš„å¥åº·æ£€æŸ¥
func (s *SyncService) HealthCheck() map[string]interface{} {
	startTime := time.Now()

	s.mu.RLock()
	status := s.status
	s.mu.RUnlock()

	// æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæ€§èƒ½
	dbHealth := s.checkDatabaseHealth()

	// æ£€æŸ¥Tronå®¢æˆ·ç«¯è¿æ¥å’Œæ€§èƒ½
	tronHealth := s.checkTronClientHealth()

	// æ£€æŸ¥å¹¶å‘åŒæ­¥ç®¡ç†å™¨çŠ¶æ€
	concurrentHealth := s.checkConcurrentSyncHealth()

	// è·å–å¤±è´¥åŒºå—è¯¦ç»†ä¿¡æ¯
	failedBlocksInfo := s.getFailedBlocksInfo()

	// è·å–åŒæ­¥è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡
	syncProgress := s.getDetailedSyncProgress()

	// è·å–é”™è¯¯ç»Ÿè®¡
	errorStats := s.getErrorStatistics()

	// è·å–æ€§èƒ½æŒ‡æ ‡
	performanceMetrics := s.getPerformanceMetrics()

	// ç¡®å®šæ•´ä½“å¥åº·çŠ¶æ€å’Œå‘Šè­¦çº§åˆ«
	overallStatus, alertLevel := s.determineHealthStatus(dbHealth, tronHealth, concurrentHealth, failedBlocksInfo, errorStats)

	result := map[string]interface{}{
		"sync_service": map[string]interface{}{
			"status":              status,
			"overall_status":      overallStatus,
			"alert_level":         alertLevel,
			"database":            dbHealth,
			"tron_client":         tronHealth,
			"concurrent_sync":     concurrentHealth,
			"failed_blocks":       failedBlocksInfo,
			"sync_progress":       syncProgress,
			"error_statistics":    errorStats,
			"performance_metrics": performanceMetrics,
			"last_updated":        time.Now(),
			"uptime":              time.Since(s.syncProgress.StartTime),
		},
	}

	// è®¡ç®—å¥åº·æ£€æŸ¥å“åº”æ—¶é—´
	responseTime := time.Since(startTime)

	// è®°å½•è¯¦ç»†çš„å¥åº·æ£€æŸ¥æ—¥å¿—
	s.logDetailedHealth(overallStatus, alertLevel, responseTime, result)

	// æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å‘Šè­¦
	s.checkAndSendAlerts(alertLevel, result)

	return result
}

// checkDatabaseHealth æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€å’Œæ€§èƒ½
func (s *SyncService) checkDatabaseHealth() map[string]interface{} {
	startTime := time.Now()
	result := map[string]interface{}{
		"status":           "healthy",
		"response_time":    0,
		"connection_count": 0,
		"errors":           []string{},
	}

	errors := []string{}

	// æ£€æŸ¥æ•°æ®åº“è¿æ¥
	sqlDB, err := s.db.DB()
	if err != nil {
		errors = append(errors, "Failed to get database instance: "+err.Error())
		result["status"] = "error"
	} else {
		// æ£€æŸ¥è¿æ¥
		if err := sqlDB.Ping(); err != nil {
			errors = append(errors, "Database ping failed: "+err.Error())
			result["status"] = "error"
		}

		// è·å–è¿æ¥ç»Ÿè®¡
		stats := sqlDB.Stats()
		result["connection_count"] = stats.OpenConnections
		result["max_connections"] = stats.MaxOpenConnections
		result["idle_connections"] = stats.Idle
		result["in_use_connections"] = stats.InUse
		result["wait_count"] = stats.WaitCount
		result["wait_duration"] = stats.WaitDuration.String()

		// æ£€æŸ¥è¿æ¥æ± å¥åº·çŠ¶æ€
		if stats.OpenConnections > int(float64(stats.MaxOpenConnections)*0.8) {
			errors = append(errors, "Database connection pool usage is high")
			result["status"] = "warning"
		}

		// æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•æ€§èƒ½
		var count int64
		queryStart := time.Now()
		if err := s.db.Model(&database.Transfer{}).Count(&count).Error; err != nil {
			errors = append(errors, "Database query test failed: "+err.Error())
			result["status"] = "error"
		} else {
			queryTime := time.Since(queryStart)
			result["query_time"] = queryTime.String()
			result["total_transfers"] = count

			// æ£€æŸ¥æŸ¥è¯¢æ€§èƒ½
			if queryTime > 1*time.Second {
				errors = append(errors, "Database query response time is slow")
				if result["status"] == "healthy" {
					result["status"] = "warning"
				}
			}
		}
	}

	result["response_time"] = time.Since(startTime).String()
	result["errors"] = errors
	return result
}

// checkTronClientHealth æ£€æŸ¥Tronå®¢æˆ·ç«¯å¥åº·çŠ¶æ€å’Œæ€§èƒ½
func (s *SyncService) checkTronClientHealth() map[string]interface{} {
	startTime := time.Now()
	result := map[string]interface{}{
		"status":        "healthy",
		"response_time": 0,
		"node_status":   map[string]interface{}{},
		"errors":        []string{},
	}

	errors := []string{}

	// æ£€æŸ¥ä¸»èŠ‚ç‚¹è¿æ¥
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := s.tronClient.HealthCheck(ctx); err != nil {
		errors = append(errors, "Primary Tron node health check failed: "+err.Error())
		result["status"] = "error"
	}

	// è·å–æœ€æ–°åŒºå—å·æµ‹è¯•æ€§èƒ½
	blockStart := time.Now()
	latestBlock, err := s.tronClient.GetLatestBlockNumber(ctx)
	if err != nil {
		errors = append(errors, "Failed to get latest block: "+err.Error())
		result["status"] = "error"
	} else {
		blockTime := time.Since(blockStart)
		result["latest_block"] = latestBlock
		result["block_query_time"] = blockTime.String()

		// æ£€æŸ¥åŒºå—æŸ¥è¯¢æ€§èƒ½
		if blockTime > 5*time.Second {
			errors = append(errors, "Tron node response time is slow")
			if result["status"] == "healthy" {
				result["status"] = "warning"
			}
		}
	}

	// æ£€æŸ¥å¤‡ç”¨èŠ‚ç‚¹çŠ¶æ€ï¼ˆå¦‚æœé…ç½®äº†å¤šèŠ‚ç‚¹ï¼‰
	nodeStatuses := make(map[string]interface{})
	if s.config.TronNodes != nil && len(s.config.TronNodes) > 1 {
		for i, nodeURL := range s.config.TronNodes {
			nodeKey := fmt.Sprintf("node_%d", i)
			nodeStatus := map[string]interface{}{
				"url":    nodeURL,
				"status": "unknown",
			}

			// è¿™é‡Œå¯ä»¥æ·»åŠ å¯¹æ¯ä¸ªèŠ‚ç‚¹çš„å¥åº·æ£€æŸ¥
			// ç”±äºæ—¶é—´é™åˆ¶ï¼Œæš‚æ—¶æ ‡è®°ä¸ºå¯ç”¨
			nodeStatus["status"] = "available"
			nodeStatuses[nodeKey] = nodeStatus
		}
	}
	result["node_status"] = nodeStatuses

	result["response_time"] = time.Since(startTime).String()
	result["errors"] = errors
	return result
}

// checkConcurrentSyncHealth æ£€æŸ¥å¹¶å‘åŒæ­¥ç®¡ç†å™¨å¥åº·çŠ¶æ€
func (s *SyncService) checkConcurrentSyncHealth() map[string]interface{} {
	result := map[string]interface{}{
		"enabled": s.concurrentEnabled,
		"status":  "healthy",
		"errors":  []string{},
	}

	if !s.concurrentEnabled {
		result["status"] = "disabled"
		return result
	}

	errors := []string{}

	if s.concurrentManager == nil {
		errors = append(errors, "Concurrent sync manager is not initialized")
		result["status"] = "error"
	} else {
		// æ£€æŸ¥å·¥ä½œåç¨‹çŠ¶æ€
		result["worker_count"] = s.concurrentManager.workerCount

		// æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€
		if s.concurrentManager.taskChan != nil {
			result["task_queue_length"] = len(s.concurrentManager.taskChan)
			result["task_queue_capacity"] = cap(s.concurrentManager.taskChan)

			// æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦æ¥è¿‘æ»¡è½½
			if len(s.concurrentManager.taskChan) > cap(s.concurrentManager.taskChan)*8/10 {
				errors = append(errors, "Task queue is nearly full")
				result["status"] = "warning"
			}
		}

		// æ£€æŸ¥ç»“æœé˜Ÿåˆ—çŠ¶æ€
		if s.concurrentManager.resultChan != nil {
			result["result_queue_length"] = len(s.concurrentManager.resultChan)
			result["result_queue_capacity"] = cap(s.concurrentManager.resultChan)

			// æ£€æŸ¥ç»“æœé˜Ÿåˆ—æ˜¯å¦ç§¯å‹
			if len(s.concurrentManager.resultChan) > cap(s.concurrentManager.resultChan)*8/10 {
				errors = append(errors, "Result queue is nearly full")
				result["status"] = "warning"
			}
		}
	}

	result["errors"] = errors
	return result
}

// getFailedBlocksInfo è·å–å¤±è´¥åŒºå—è¯¦ç»†ä¿¡æ¯
func (s *SyncService) getFailedBlocksInfo() map[string]interface{} {
	s.failedMutex.RLock()
	defer s.failedMutex.RUnlock()

	failedCount := len(s.failedBlocks)
	result := map[string]interface{}{
		"total_count": failedCount,
		"blocks":      []map[string]interface{}{},
		"status":      "healthy",
	}

	// å¦‚æœå¤±è´¥åŒºå—è¿‡å¤šï¼Œæ ‡è®°ä¸ºè­¦å‘Š
	if failedCount > 10 {
		result["status"] = "warning"
	} else if failedCount > 50 {
		result["status"] = "error"
	}

	// æ”¶é›†å¤±è´¥åŒºå—è¯¦ç»†ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤º10ä¸ªï¼‰
	blocks := []map[string]interface{}{}
	count := 0
	for blockNum, failedBlock := range s.failedBlocks {
		if count >= 10 {
			break
		}
		blocks = append(blocks, map[string]interface{}{
			"block_number": blockNum,
			"fail_count":   failedBlock.FailCount,
			"last_attempt": failedBlock.LastAttempt,
			"last_error":   failedBlock.LastError,
		})
		count++
	}
	result["blocks"] = blocks

	return result
}

// getDetailedSyncProgress è·å–è¯¦ç»†çš„åŒæ­¥è¿›åº¦ä¿¡æ¯
func (s *SyncService) getDetailedSyncProgress() map[string]interface{} {
	s.progressMu.RLock()
	defer s.progressMu.RUnlock()

	if s.syncProgress == nil {
		return map[string]interface{}{
			"status": "not_initialized",
		}
	}

	// è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
	progressPercent := float64(0)
	if s.syncProgress.TargetBlock > s.syncProgress.CurrentBlock {
		progressPercent = float64(s.syncProgress.ProcessedBlocks) / float64(s.syncProgress.TargetBlock-s.syncProgress.CurrentBlock) * 100
	}

	// è®¡ç®—åŒæ­¥é€Ÿç‡ï¼ˆåŒºå—/ç§’ï¼‰
	elapsed := time.Since(s.syncProgress.StartTime)
	syncRate := float64(0)
	if elapsed.Seconds() > 0 {
		syncRate = float64(s.syncProgress.ProcessedBlocks) / elapsed.Seconds()
	}

	// ä¼°ç®—å‰©ä½™æ—¶é—´
	remainingBlocks := s.syncProgress.TargetBlock - s.syncProgress.CurrentBlock - s.syncProgress.ProcessedBlocks
	estimatedTime := time.Duration(0)
	if syncRate > 0 {
		estimatedTime = time.Duration(float64(remainingBlocks)/syncRate) * time.Second
	}

	return map[string]interface{}{
		"current_block":    s.syncProgress.CurrentBlock,
		"target_block":     s.syncProgress.TargetBlock,
		"processed_blocks": s.syncProgress.ProcessedBlocks,
		"failed_blocks":    s.syncProgress.FailedBlocks,
		"progress_percent": fmt.Sprintf("%.2f%%", progressPercent),
		"sync_rate":        fmt.Sprintf("%.2f blocks/sec", syncRate),
		"estimated_time":   estimatedTime.String(),
		"active_workers":   s.syncProgress.ActiveWorkers,
		"total_tasks":      s.syncProgress.TotalTasks,
		"completed_tasks":  s.syncProgress.CompletedTasks,
		"start_time":       s.syncProgress.StartTime,
		"last_update":      s.syncProgress.LastUpdateTime,
		"elapsed_time":     elapsed.String(),
	}
}

// getErrorStatistics è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
func (s *SyncService) getErrorStatistics() map[string]interface{} {
	s.mu.RLock()
	errorCount := s.status.ErrorCount
	lastError := s.status.LastError
	s.mu.RUnlock()

	// è·å–å…¨å±€é”™è¯¯ç»Ÿè®¡
	globalErrorStats := utils.GetErrorStats()

	return map[string]interface{}{
		"sync_errors":     errorCount,
		"last_sync_error": lastError,
		"global_errors":   globalErrorStats,
		"error_rate":      s.calculateErrorRate(),
	}
}

// calculateErrorRate è®¡ç®—é”™è¯¯ç‡
func (s *SyncService) calculateErrorRate() float64 {
	s.mu.RLock()
	errorCount := s.status.ErrorCount
	syncedCount := s.status.SyncedCount
	s.mu.RUnlock()

	totalOperations := errorCount + syncedCount
	if totalOperations == 0 {
		return 0
	}

	return float64(errorCount) / float64(totalOperations) * 100
}

// getPerformanceMetrics è·å–æ€§èƒ½æŒ‡æ ‡
func (s *SyncService) getPerformanceMetrics() map[string]interface{} {
	s.mu.RLock()
	lastSyncTime := s.status.LastSyncTime
	syncInterval := s.status.SyncInterval
	syncedCount := s.status.SyncedCount
	s.mu.RUnlock()

	// è®¡ç®—åŒæ­¥é¢‘ç‡
	uptime := time.Since(s.syncProgress.StartTime)
	syncFrequency := float64(0)
	if uptime.Hours() > 0 {
		syncFrequency = float64(syncedCount) / uptime.Hours()
	}

	return map[string]interface{}{
		"last_sync_time": lastSyncTime,
		"sync_interval":  syncInterval.String(),
		"synced_count":   syncedCount,
		"sync_frequency": fmt.Sprintf("%.2f syncs/hour", syncFrequency),
		"uptime":         uptime.String(),
		"memory_usage":   s.getMemoryUsage(),
	}
}

// getMemoryUsage è·å–å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆç®€åŒ–ç‰ˆï¼‰
func (s *SyncService) getMemoryUsage() map[string]interface{} {
	s.processedMu.RLock()
	processedBlocksCount := len(s.processedBlocks)
	s.processedMu.RUnlock()

	s.failedMutex.RLock()
	failedBlocksCount := len(s.failedBlocks)
	s.failedMutex.RUnlock()

	return map[string]interface{}{
		"processed_blocks_cache": processedBlocksCount,
		"failed_blocks_cache":    failedBlocksCount,
	}
}

// determineHealthStatus ç¡®å®šæ•´ä½“å¥åº·çŠ¶æ€å’Œå‘Šè­¦çº§åˆ«
func (s *SyncService) determineHealthStatus(dbHealth, tronHealth, concurrentHealth, failedBlocksInfo, errorStats map[string]interface{}) (string, string) {
	// æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
	dbStatus := dbHealth["status"].(string)
	tronStatus := tronHealth["status"].(string)
	concurrentStatus := concurrentHealth["status"].(string)
	failedBlocksStatus := failedBlocksInfo["status"].(string)

	// è®¡ç®—é”™è¯¯ç‡
	errorRate := errorStats["error_rate"].(float64)

	// ç¡®å®šæ•´ä½“çŠ¶æ€
	if dbStatus == "error" || tronStatus == "error" || concurrentStatus == "error" {
		return "unhealthy", "critical"
	}

	if dbStatus == "warning" || tronStatus == "warning" || concurrentStatus == "warning" || failedBlocksStatus == "warning" || errorRate > 5 {
		return "degraded", "warning"
	}

	if failedBlocksStatus == "error" || errorRate > 10 {
		return "unhealthy", "major"
	}

	return "healthy", "none"
}

// logDetailedHealth è®°å½•è¯¦ç»†çš„å¥åº·æ£€æŸ¥æ—¥å¿—ï¼ˆä»…åœ¨çŠ¶æ€å˜åŒ–æ—¶è®°å½•ï¼‰
func (s *SyncService) logDetailedHealth(overallStatus, alertLevel string, responseTime time.Duration, result map[string]interface{}) {
	// æ£€æŸ¥çŠ¶æ€æ˜¯å¦å‘ç”Ÿå˜åŒ–
	statusChanged := s.lastHealthStatus != overallStatus || s.lastAlertLevel != alertLevel
	
	// åªåœ¨çŠ¶æ€å˜åŒ–æ—¶è®°å½•è¯¦ç»†æ—¥å¿—
	if statusChanged {
		// è®°å½•åŸºç¡€å¥åº·æ—¥å¿—
		utils.LogHealth("sync_service", overallStatus, responseTime, result)

		// æ ¹æ®å‘Šè­¦çº§åˆ«è®°å½•ä¸åŒçº§åˆ«çš„æ—¥å¿—
		switch alertLevel {
		case "critical":
			utils.LogWithFields(utils.ERROR, "Critical health issue detected in sync service", map[string]interface{}{
				"alert_level":   alertLevel,
				"status":        overallStatus,
				"response_time": responseTime.String(),
				"previous_status": s.lastHealthStatus,
				"previous_alert":  s.lastAlertLevel,
			})
		case "major":
			utils.LogWithFields(utils.WARN, "Major health issue detected in sync service", map[string]interface{}{
				"alert_level":   alertLevel,
				"status":        overallStatus,
				"response_time": responseTime.String(),
				"previous_status": s.lastHealthStatus,
				"previous_alert":  s.lastAlertLevel,
			})
		case "warning":
			utils.LogWithFields(utils.WARN, "Health warning detected in sync service", map[string]interface{}{
				"alert_level":   alertLevel,
				"status":        overallStatus,
				"response_time": responseTime.String(),
				"previous_status": s.lastHealthStatus,
				"previous_alert":  s.lastAlertLevel,
			})
		default:
			// åªæœ‰ä»å¼‚å¸¸çŠ¶æ€æ¢å¤åˆ°æ­£å¸¸çŠ¶æ€æ—¶æ‰è®°å½•INFOæ—¥å¿—
			if s.lastAlertLevel != "" && s.lastAlertLevel != "none" {
				utils.LogWithFields(utils.INFO, "Sync service health recovered to normal", map[string]interface{}{
					"status":        overallStatus,
					"response_time": responseTime.String(),
					"previous_status": s.lastHealthStatus,
					"previous_alert":  s.lastAlertLevel,
				})
			}
		}
		
		// æ›´æ–°ä¸Šæ¬¡çŠ¶æ€
		s.lastHealthStatus = overallStatus
		s.lastAlertLevel = alertLevel
	}
}

// checkAndSendAlerts æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å‘Šè­¦
func (s *SyncService) checkAndSendAlerts(alertLevel string, healthData map[string]interface{}) {
	// è¿™é‡Œå¯ä»¥é›†æˆå‘Šè­¦ç³»ç»Ÿï¼Œå¦‚é‚®ä»¶ã€çŸ­ä¿¡ã€Webhookç­‰
	switch alertLevel {
	case "critical":
		utils.LogWithFields(utils.ERROR, "ALERT: Critical sync service issue requires immediate attention", map[string]interface{}{
			"alert_type":      "critical",
			"health_data":     healthData,
			"action_required": "immediate_intervention",
		})
		// è¿™é‡Œå¯ä»¥å‘é€ç´§æ€¥å‘Šè­¦

	case "major":
		utils.LogWithFields(utils.WARN, "ALERT: Major sync service issue detected", map[string]interface{}{
			"alert_type":      "major",
			"health_data":     healthData,
			"action_required": "investigation_needed",
		})
		// è¿™é‡Œå¯ä»¥å‘é€ä¸»è¦å‘Šè­¦

	case "warning":
		utils.LogWithFields(utils.WARN, "ALERT: Sync service performance degradation", map[string]interface{}{
			"alert_type":      "warning",
			"health_data":     healthData,
			"action_required": "monitoring_recommended",
		})
		// è¿™é‡Œå¯ä»¥å‘é€è­¦å‘Šå‘Šè­¦
	}
}

// å¤±è´¥åŒºå—ç®¡ç†æ–¹æ³•

// retryFailedBlocks é‡è¯•å¤±è´¥çš„åŒºå—
func (s *SyncService) retryFailedBlocks(ctx context.Context) error {
	s.failedMutex.RLock()
	failedBlocks := make([]*FailedBlock, 0, len(s.failedBlocks))
	for _, block := range s.failedBlocks {
		// æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•ï¼ˆè·ç¦»ä¸Šæ¬¡å°è¯•è‡³å°‘5åˆ†é’Ÿï¼‰
		if time.Since(block.LastAttempt) >= 5*time.Minute && block.FailCount < 5 {
			failedBlocks = append(failedBlocks, block)
		}
	}
	s.failedMutex.RUnlock()

	if len(failedBlocks) == 0 {
		return nil
	}

	utils.Info("Retrying %d failed blocks", len(failedBlocks))

	for _, block := range failedBlocks {
		transfers, err := s.syncBlockWithRetry(ctx, block.BlockNumber)
		if err != nil {
			// æ›´æ–°å¤±è´¥æ¬¡æ•°
			s.updateFailedBlock(block.BlockNumber, err.Error())
			continue
		}

		// ä¿å­˜è½¬è´¦æ•°æ®
		for _, transfer := range transfers {
			err := s.saveTransferToDatabase(transfer)
			if err != nil {
				utils.Error("Failed to save transfer %s: %v", transfer.TxHash, err)
				continue
			}
		}

		// ç§»é™¤æˆåŠŸçš„åŒºå—
		s.removeFailedBlock(block.BlockNumber)
		utils.Info("Successfully retried block %d", block.BlockNumber)
	}

	return nil
}

// syncBlockWithRetry å¸¦é‡è¯•æœºåˆ¶çš„åŒºå—åŒæ­¥
func (s *SyncService) syncBlockWithRetry(ctx context.Context, blockNum uint64) ([]*blockchain.TransferEvent, error) {
	maxRetries := 3
	baseDelay := time.Second

	for attempt := 1; attempt <= maxRetries; attempt++ {
		transfers, err := s.tronClient.GetUSDTTransfersByBlock(ctx, blockNum)
		if err == nil {
			return transfers, nil
		}

		utils.Error("Attempt %d/%d failed for block %d: %v", attempt, maxRetries, blockNum, err)

		// å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
		if attempt < maxRetries {
			delay := time.Duration(attempt) * baseDelay
			select {
			case <-ctx.Done():
				return nil, ctx.Err()
			case <-time.After(delay):
			}
		}
	}

	return nil, fmt.Errorf("failed to sync block %d after %d attempts", blockNum, maxRetries)
}

// recordFailedBlock è®°å½•å¤±è´¥çš„åŒºå—
// recordFailedBlock è®°å½•å¤±è´¥åŒºå—ï¼ŒåŒ…å«è¯¦ç»†é”™è¯¯è·Ÿè¸ªå’Œå‘Šè­¦æœºåˆ¶
func (s *SyncService) recordFailedBlock(blockNum uint64, errorMsg string) {
	s.failedMutex.Lock()
	defer s.failedMutex.Unlock()

	// è®°å½•å¤±è´¥åŒºå—ä¿¡æ¯
	if existing, exists := s.failedBlocks[blockNum]; exists {
		existing.FailCount++
		existing.LastAttempt = time.Now()
		existing.LastError = errorMsg

		// é‡å¤å¤±è´¥çš„è¯¦ç»†æ—¥å¿—
		utils.Warn("Block %d failed again (attempt %d): %s", blockNum, existing.FailCount, errorMsg)

		// æ£€æŸ¥æ˜¯å¦ä¸ºä¸¥é‡é”™è¯¯ï¼ˆå¤±è´¥æ¬¡æ•°è¿‡å¤šï¼‰
		if existing.FailCount >= 5 {
			utils.Error("Critical: Block %d has failed %d times, last error: %s", blockNum, existing.FailCount, errorMsg)
		}
	} else {
		s.failedBlocks[blockNum] = &FailedBlock{
			BlockNumber: blockNum,
			FailCount:   1,
			LastAttempt: time.Now(),
			LastError:   errorMsg,
		}

		// é¦–æ¬¡å¤±è´¥çš„è¯¦ç»†æ—¥å¿—
		utils.Info("New failed block recorded: %d, error: %s", blockNum, errorMsg)
	}

	// æ›´æ–°é”™è¯¯ç»Ÿè®¡
	s.updateErrorCount()

	// æ£€æŸ¥å¹¶å‘é€å‘Šè­¦
	s.checkFailedBlockAlerts(blockNum, s.failedBlocks[blockNum])
}

// checkFailedBlockAlerts æ£€æŸ¥å¤±è´¥åŒºå—å‘Šè­¦
func (s *SyncService) checkFailedBlockAlerts(blockNum uint64, failedBlock *FailedBlock) {
	// æ£€æŸ¥å•ä¸ªåŒºå—å¤±è´¥æ¬¡æ•°å‘Šè­¦
	if failedBlock.FailCount == 3 {
		utils.Warn("Alert: Block %d has failed %d times, requires attention", blockNum, failedBlock.FailCount)
	} else if failedBlock.FailCount >= 10 {
		utils.Error("Critical Alert: Block %d has failed %d times, manual intervention required", blockNum, failedBlock.FailCount)
	}

	// æ£€æŸ¥æ€»å¤±è´¥åŒºå—æ•°é‡å‘Šè­¦
	totalFailedBlocks := len(s.failedBlocks)
	if totalFailedBlocks >= 100 {
		utils.Error("Critical Alert: Total failed blocks count reached %d, system health degraded", totalFailedBlocks)
	} else if totalFailedBlocks >= 50 {
		utils.Warn("Warning: Total failed blocks count reached %d, monitoring required", totalFailedBlocks)
	}

	// æ£€æŸ¥é”™è¯¯ç‡å‘Šè­¦
	errorRate := s.calculateErrorRate()
	if errorRate >= 0.1 { // 10%é”™è¯¯ç‡
		utils.Error("Critical Alert: Error rate reached %.2f%%, system performance severely impacted", errorRate*100)
	} else if errorRate >= 0.05 { // 5%é”™è¯¯ç‡
		utils.Warn("Warning: Error rate reached %.2f%%, performance monitoring required", errorRate*100)
	}
}

// updateFailedBlock æ›´æ–°å¤±è´¥åŒºå—ä¿¡æ¯
func (s *SyncService) updateFailedBlock(blockNum uint64, errorMsg string) {
	s.failedMutex.Lock()
	defer s.failedMutex.Unlock()

	if block, exists := s.failedBlocks[blockNum]; exists {
		block.FailCount++
		block.LastAttempt = time.Now()
		block.LastError = errorMsg
	}
}

// removeFailedBlock ç§»é™¤å¤±è´¥åŒºå—è®°å½•
func (s *SyncService) removeFailedBlock(blockNum uint64) {
	s.failedMutex.Lock()
	defer s.failedMutex.Unlock()
	delete(s.failedBlocks, blockNum)
}

// findLastSuccessBlock æŸ¥æ‰¾æœ€åä¸€ä¸ªæˆåŠŸçš„åŒºå—
func (s *SyncService) findLastSuccessBlock(startBlock, endBlock uint64, failedBlocks []uint64) uint64 {
	// åˆ›å»ºå¤±è´¥åŒºå—çš„æ˜ å°„ä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
	failedMap := make(map[uint64]bool)
	for _, block := range failedBlocks {
		failedMap[block] = true
	}

	// ä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€ä¸ªæˆåŠŸçš„åŒºå—
	for block := endBlock; block >= startBlock; block-- {
		if !failedMap[block] {
			return block
		}
	}

	// å¦‚æœæ‰€æœ‰åŒºå—éƒ½å¤±è´¥äº†ï¼Œè¿”å›èµ·å§‹åŒºå—çš„å‰ä¸€ä¸ª
	if startBlock > 0 {
		return startBlock - 1
	}
	return 0
}

// updateErrorCount æ›´æ–°é”™è¯¯è®¡æ•°
func (s *SyncService) updateErrorCount() {
	s.mu.Lock()
	s.status.ErrorCount++
	s.mu.Unlock()
}

// GetFailedBlocks è·å–å¤±è´¥åŒºå—åˆ—è¡¨
func (s *SyncService) GetFailedBlocks() []*FailedBlock {
	s.failedMutex.RLock()
	defer s.failedMutex.RUnlock()

	blocks := make([]*FailedBlock, 0, len(s.failedBlocks))
	for _, block := range s.failedBlocks {
		blocks = append(blocks, block)
	}
	return blocks
}

// GetSyncProgress è·å–è¯¦ç»†çš„åŒæ­¥è¿›åº¦ä¿¡æ¯
func (s *SyncService) GetSyncProgress() map[string]interface{} {
	progress := s.getSyncProgress()

	s.failedMutex.RLock()
	failedBlocks := make([]*FailedBlock, 0, len(s.failedBlocks))
	for _, fb := range s.failedBlocks {
		failedBlocks = append(failedBlocks, fb)
	}
	s.failedMutex.RUnlock()

	// è®¡ç®—å®Œæˆç™¾åˆ†æ¯”
	var completionPercentage float64
	if progress.TargetBlock > 0 {
		completionPercentage = float64(progress.CurrentBlock) / float64(progress.TargetBlock) * 100
	}

	return map[string]interface{}{
		"current_block":         progress.CurrentBlock,
		"target_block":          progress.TargetBlock,
		"processed_blocks":      progress.ProcessedBlocks,
		"failed_blocks":         progress.FailedBlocks,
		"completion_percentage": completionPercentage,
		"sync_rate":             progress.SyncRate,
		"estimated_time":        progress.EstimatedTime.String(),
		"active_workers":        progress.ActiveWorkers,
		"total_tasks":           progress.TotalTasks,
		"completed_tasks":       progress.CompletedTasks,
		"start_time":            progress.StartTime,
		"last_update_time":      progress.LastUpdateTime,
		"failed_block_details":  failedBlocks,
		"concurrent_enabled":    s.concurrentEnabled,
	}
}

// addFailedBlock æ·»åŠ å¤±è´¥åŒºå—åˆ°é‡è¯•é˜Ÿåˆ—
// addFailedBlock æ·»åŠ å¤±è´¥åŒºå—åˆ°é‡è¯•é˜Ÿåˆ—ï¼ŒåŒ…å«è¯¦ç»†é”™è¯¯è·Ÿè¸ª
func (s *SyncService) addFailedBlock(blockNum uint64, err error) {
	s.failedMutex.Lock()
	defer s.failedMutex.Unlock()

	errorMsg := err.Error()
	if existing, exists := s.failedBlocks[blockNum]; exists {
		existing.FailCount++
		existing.LastAttempt = time.Now()
		existing.LastError = errorMsg

		// é‡å¤å¤±è´¥çš„è¯¦ç»†æ—¥å¿—
		utils.Warn("Block %d added to retry queue again (attempt %d): %s", blockNum, existing.FailCount, errorMsg)

		// æ£€æŸ¥æ˜¯å¦ä¸ºæŒç»­å¤±è´¥
		if existing.FailCount >= 3 {
			utils.Error("Block %d has been added to retry queue %d times, persistent failure detected: %s", blockNum, existing.FailCount, errorMsg)
		}
	} else {
		s.failedBlocks[blockNum] = &FailedBlock{
			BlockNumber: blockNum,
			FailCount:   1,
			LastAttempt: time.Now(),
			LastError:   errorMsg,
		}

		// é¦–æ¬¡å¤±è´¥çš„è¯¦ç»†æ—¥å¿—
		utils.Info("New block %d added to retry queue: %s", blockNum, errorMsg)
	}

	// æ›´æ–°é”™è¯¯ç»Ÿè®¡
	s.updateErrorCount()

	// æ£€æŸ¥å¹¶å‘é€å‘Šè­¦
	s.checkFailedBlockAlerts(blockNum, s.failedBlocks[blockNum])

	// è®°å½•é”™è¯¯ç±»å‹ç»Ÿè®¡
	s.logErrorTypeStatistics(errorMsg)
}

// å¹¶å‘åŒæ­¥ç®¡ç†å™¨ç›¸å…³æ–¹æ³•

// logErrorTypeStatistics è®°å½•é”™è¯¯ç±»å‹ç»Ÿè®¡
func (s *SyncService) logErrorTypeStatistics(errorMsg string) {
	// åªåœ¨å¼€å‘æ¨¡å¼ä¸‹è®°å½•Debugçº§åˆ«çš„é”™è¯¯ç»Ÿè®¡æ—¥å¿—
	isDevMode := strings.ToUpper(s.config.LogLevel) == "DEBUG"
	
	if !isDevMode {
		return // éå¼€å‘æ¨¡å¼ä¸‹è·³è¿‡Debugæ—¥å¿—
	}
	
	// åˆ†æé”™è¯¯ç±»å‹å¹¶è®°å½•ç»Ÿè®¡ï¼ˆä»…åœ¨å¼€å‘æ¨¡å¼ä¸‹ï¼‰
	if strings.Contains(errorMsg, "timeout") || strings.Contains(errorMsg, "deadline") {
		utils.Debug("Timeout error detected: %s", errorMsg)
	} else if strings.Contains(errorMsg, "connection") || strings.Contains(errorMsg, "network") {
		utils.FileLog(utils.DEBUG, "Network error detected: %s", errorMsg)
	} else if strings.Contains(errorMsg, "parse") || strings.Contains(errorMsg, "unmarshal") {
		utils.FileLog(utils.DEBUG, "Data parsing error detected: %s", errorMsg)
	} else if strings.Contains(errorMsg, "database") || strings.Contains(errorMsg, "sql") {
		utils.FileLog(utils.DEBUG, "Database error detected: %s", errorMsg)
	} else if strings.Contains(errorMsg, "rate limit") || strings.Contains(errorMsg, "too many requests") {
		utils.FileLog(utils.DEBUG, "Rate limiting error detected: %s", errorMsg)
	} else {
		utils.FileLog(utils.DEBUG, "Unknown error type detected: %s", errorMsg)
	}
}

// newConcurrentSyncManager åˆ›å»ºæ–°çš„å¹¶å‘åŒæ­¥ç®¡ç†å™¨
func (s *SyncService) newConcurrentSyncManager() *ConcurrentSyncManager {
	ctx, cancel := context.WithCancel(context.Background())
	return &ConcurrentSyncManager{
		taskChan:    make(chan *SyncTask, s.config.ConcurrentConfig.BufferSize),
		resultChan:  make(chan *SyncResult, s.config.ConcurrentConfig.BufferSize),
		workerCount: s.config.ConcurrentConfig.WorkerCount,
		tronClient:  s.tronClient,
		ctx:         ctx,
		cancel:      cancel,
	}
}

// startConcurrentSync å¯åŠ¨å¹¶å‘åŒæ­¥
func (s *SyncService) startConcurrentSync() {
	if s.concurrentManager == nil {
		utils.Error("å¹¶å‘ç®¡ç†å™¨ä¸ºç©ºï¼Œæ— æ³•å¯åŠ¨å¹¶å‘åŒæ­¥")
		return
	}

	utils.Info("å¯åŠ¨å¹¶å‘åŒæ­¥ï¼Œå·¥ä½œåç¨‹æ•°: %d", s.concurrentManager.workerCount)

	// å¯åŠ¨å·¥ä½œåç¨‹
	for i := 0; i < s.concurrentManager.workerCount; i++ {
		s.concurrentManager.wg.Add(1)
		go s.syncWorker(i)
	}

	// å¯åŠ¨ç»“æœå¤„ç†åç¨‹
	s.concurrentManager.wg.Add(1)
	go s.resultProcessor()

	utils.Info("å¹¶å‘åŒæ­¥å¯åŠ¨å®Œæˆ")
}

// stopConcurrentSync åœæ­¢å¹¶å‘åŒæ­¥
func (s *SyncService) stopConcurrentSync() {
	if s.concurrentManager == nil {
		return
	}

	// å–æ¶ˆä¸Šä¸‹æ–‡
	s.concurrentManager.cancel()

	// å…³é—­ä»»åŠ¡é€šé“
	close(s.concurrentManager.taskChan)

	// ç­‰å¾…æ‰€æœ‰å·¥ä½œåç¨‹å®Œæˆ
	s.concurrentManager.wg.Wait()

	// å…³é—­ç»“æœé€šé“
	close(s.concurrentManager.resultChan)
}

// syncWorker åŒæ­¥å·¥ä½œåç¨‹
func (s *SyncService) syncWorker(workerID int) {
	defer s.concurrentManager.wg.Done()

	utils.Info("Sync worker %d started", workerID)
	defer utils.Info("Sync worker %d stopped", workerID)

	for {
		select {
		case <-s.concurrentManager.ctx.Done():
			return
		case task, ok := <-s.concurrentManager.taskChan:
			if !ok {
				return
			}

			// å¤„ç†åŒæ­¥ä»»åŠ¡
			result := s.processSyncTask(task, workerID)

			// å‘é€ç»“æœ
			select {
			case s.concurrentManager.resultChan <- result:
			case <-s.concurrentManager.ctx.Done():
				return
			}
		}
	}
}

// processSyncTask å¤„ç†åŒæ­¥ä»»åŠ¡ï¼ˆé›†æˆæ•…éšœè½¬ç§»æœºåˆ¶ï¼‰
func (s *SyncService) processSyncTask(task *SyncTask, workerID int) *SyncResult {
	startTime := time.Now()
	result := &SyncResult{
		Task:     task,
		Duration: 0,
	}

	// è·å–èŠ‚ç‚¹ä¿¡æ¯ç”¨äºæ—¥å¿—æ˜¾ç¤º
	nodeURL := "unknown"
	if task.NodeIndex < len(s.config.TronNodes) {
		nodeURL = s.config.TronNodes[task.NodeIndex].URL
	}

	utils.Info("ğŸ”„ Worker %d processing blocks %d-%d using node %d (%s)",
		workerID, task.StartBlock, task.EndBlock, task.NodeIndex, nodeURL)

	// åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
	ctx, cancel := context.WithTimeout(s.concurrentManager.ctx, s.config.ConcurrentConfig.SyncTimeout)
	defer cancel()

	// è·å–åŒºå—èŒƒå›´å†…çš„USDTè½¬è´¦äº‹ä»¶ï¼ˆå¸¦æ•…éšœè½¬ç§»ï¼‰
	var allTransfers []*blockchain.TransferEvent
	for blockNum := task.StartBlock; blockNum <= task.EndBlock; blockNum++ {
		utils.FileLog(utils.DEBUG, "ğŸ“¡ Worker %d fetching block %d from node %d (%s)", workerID, blockNum, task.NodeIndex, nodeURL)
		transfers, err := s.getTransfersWithFailover(ctx, blockNum, task.NodeIndex, workerID)
		if err != nil {
			utils.FileLog(utils.ERROR, "âŒ Worker %d failed to get transfers for block %d after failover: %v", workerID, blockNum, err)
			result.Error = fmt.Errorf("failed to get transfers for block %d: %w", blockNum, err)
			result.Duration = time.Since(startTime)
			return result
		}
		utils.FileLog(utils.DEBUG, "âœ… Worker %d successfully fetched %d transfers from block %d", workerID, len(transfers), blockNum)
		allTransfers = append(allTransfers, transfers...)
	}

	result.Transfers = allTransfers
	result.Duration = time.Since(startTime)
	utils.FileLog(utils.INFO, "âœ… Worker %d completed blocks %d-%d using node %d (%s), found %d transfers in %v",
		workerID, task.StartBlock, task.EndBlock, task.NodeIndex, nodeURL, len(allTransfers), result.Duration)

	return result
}

// getTransfersWithFailover å¸¦æ•…éšœè½¬ç§»çš„è·å–è½¬è´¦æ•°æ®
func (s *SyncService) getTransfersWithFailover(ctx context.Context, blockNum uint64, preferredNodeIndex, workerID int) ([]*blockchain.TransferEvent, error) {
	nodeCount := len(s.config.TronNodes)
	if nodeCount <= 1 {
		// å•èŠ‚ç‚¹æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰å®¢æˆ·ç«¯
		utils.Debug("ğŸ”— Worker %d: Single node mode, fetching block %d", workerID, blockNum)
		return s.tronClient.GetUSDTTransfersByBlock(ctx, blockNum)
	}

	// å¤šèŠ‚ç‚¹æ¨¡å¼ï¼Œå®ç°æ•…éšœè½¬ç§»
	maxRetries := s.config.RetryConfig.MaxRetries
	if maxRetries == 0 {
		maxRetries = 3
	}

	// è·å–é¦–é€‰èŠ‚ç‚¹ä¿¡æ¯
	preferredNodeURL := "unknown"
	if preferredNodeIndex < len(s.config.TronNodes) {
		preferredNodeURL = s.config.TronNodes[preferredNodeIndex].URL
	}

	utils.Debug("ğŸ¯ Worker %d: Multi-node mode, trying preferred node %d (%s) for block %d",
		workerID, preferredNodeIndex, preferredNodeURL, blockNum)

	// å°è¯•é¦–é€‰èŠ‚ç‚¹
	transfers, err := s.tryGetTransfersFromNode(ctx, blockNum, preferredNodeIndex, workerID)
	if err == nil {
		utils.Debug("âœ… Worker %d: Preferred node %d (%s) successfully fetched block %d",
			workerID, preferredNodeIndex, preferredNodeURL, blockNum)
		return transfers, nil
	}

	utils.Warn("âš ï¸ Worker %d: Node %d (%s) failed for block %d, initiating failover: %v",
		workerID, preferredNodeIndex, preferredNodeURL, blockNum, err)

	// æ•…éšœè½¬ç§»ï¼šå°è¯•å…¶ä»–èŠ‚ç‚¹
	for attempt := 1; attempt <= maxRetries; attempt++ {
		// é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆè½®è¯¢ï¼‰
		nextNodeIndex := (preferredNodeIndex + attempt) % nodeCount
		nextNodeURL := "unknown"
		if nextNodeIndex < len(s.config.TronNodes) {
			nextNodeURL = s.config.TronNodes[nextNodeIndex].URL
		}

		utils.Info("ğŸ”„ Worker %d: Failover attempt %d/%d, switching to node %d (%s) for block %d",
			workerID, attempt, maxRetries, nextNodeIndex, nextNodeURL, blockNum)

		transfers, err := s.tryGetTransfersFromNode(ctx, blockNum, nextNodeIndex, workerID)
		if err == nil {
			utils.Info("âœ… Worker %d: Failover successful! Node %d (%s) completed block %d",
				workerID, nextNodeIndex, nextNodeURL, blockNum)
			return transfers, nil
		}

		utils.Warn("âŒ Worker %d: Node %d (%s) also failed for block %d: %v",
			workerID, nextNodeIndex, nextNodeURL, blockNum, err)

		// å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
		if attempt < maxRetries {
			delay := time.Duration(attempt) * s.config.RetryConfig.InitialDelay
			utils.Debug("â³ Worker %d: Waiting %v before next failover attempt", workerID, delay)
			select {
			case <-ctx.Done():
				return nil, ctx.Err()
			case <-time.After(delay):
			}
		}
	}

	utils.Error("ğŸ’¥ Worker %d: All %d nodes failed for block %d after %d attempts", workerID, nodeCount, blockNum, maxRetries)
	return nil, fmt.Errorf("all nodes failed for block %d after %d attempts", blockNum, maxRetries)
}

// tryGetTransfersFromNode å°è¯•ä»æŒ‡å®šèŠ‚ç‚¹è·å–è½¬è´¦æ•°æ®
func (s *SyncService) tryGetTransfersFromNode(ctx context.Context, blockNum uint64, nodeIndex, workerID int) ([]*blockchain.TransferEvent, error) {
	// è·å–èŠ‚ç‚¹ä¿¡æ¯
	nodeURL := "unknown"
	if nodeIndex < len(s.config.TronNodes) {
		nodeURL = s.config.TronNodes[nodeIndex].URL
	}

	utils.Debug("ğŸŒ Worker %d: Sending request to node %d (%s) for block %d", workerID, nodeIndex, nodeURL, blockNum)
	startTime := time.Now()

	// è¿™é‡Œåº”è¯¥ä½¿ç”¨æŒ‡å®šèŠ‚ç‚¹çš„å®¢æˆ·ç«¯
	// ç”±äºå½“å‰TronHTTPClientå·²ç»å®ç°äº†å¤šèŠ‚ç‚¹æ•…éšœè½¬ç§»ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨
	// åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½éœ€è¦ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºå•ç‹¬çš„å®¢æˆ·ç«¯å®ä¾‹
	transfers, err := s.tronClient.GetUSDTTransfersByBlock(ctx, blockNum)

	duration := time.Since(startTime)
	if err != nil {
		utils.Debug("âŒ Worker %d: Node %d (%s) request failed for block %d after %v: %v",
			workerID, nodeIndex, nodeURL, blockNum, duration, err)
		return nil, err
	}

	utils.Debug("âœ… Worker %d: Node %d (%s) successfully returned %d transfers for block %d in %v",
		workerID, nodeIndex, nodeURL, len(transfers), blockNum, duration)
	return transfers, nil
}

// resultProcessor ç»“æœå¤„ç†åç¨‹
func (s *SyncService) resultProcessor() {
	defer s.concurrentManager.wg.Done()

	utils.Info("Result processor started")
	defer utils.Info("Result processor stopped")

	for {
		select {
		case <-s.concurrentManager.ctx.Done():
			return
		case result, ok := <-s.concurrentManager.resultChan:
			if !ok {
				return
			}

			// å¤„ç†åŒæ­¥ç»“æœ
			s.handleSyncResult(result)
		}
	}
}

// handleSyncResult å¤„ç†åŒæ­¥ç»“æœï¼ˆæ”¯æŒä»»åŠ¡é‡åˆ†é…å’Œè¿›åº¦è·Ÿè¸ªï¼‰
func (s *SyncService) handleSyncResult(result *SyncResult) {
	if result.Error != nil {
		utils.Error("Sync task failed for blocks %d-%d using node %d: %v",
			result.Task.StartBlock, result.Task.EndBlock, result.Task.NodeIndex, result.Error)

		// æ›´æ–°å¤±è´¥åŒºå—è®¡æ•°
		s.progressMu.Lock()
		s.syncProgress.FailedBlocks += result.Task.EndBlock - result.Task.StartBlock + 1
		s.progressMu.Unlock()

		// æ£€æŸ¥æ˜¯å¦éœ€è¦é‡åˆ†é…ä»»åŠ¡
		if s.shouldRetryTask(result.Task) {
			s.retryTaskWithDifferentNode(result.Task)
		} else {
			// è®°å½•å¤±è´¥çš„åŒºå—èŒƒå›´
			for blockNum := result.Task.StartBlock; blockNum <= result.Task.EndBlock; blockNum++ {
				s.recordFailedBlock(blockNum, result.Error.Error())
			}
		}

		s.updateErrorCount()
		return
	}

	// ä¿å­˜è½¬è´¦æ•°æ®åˆ°æ•°æ®åº“
	successCount := 0
	for _, transfer := range result.Transfers {
		err := s.saveTransferToDatabase(transfer)
		if err != nil {
			utils.Error("Failed to save transfer %s: %v", transfer.TxHash, err)
			// è®°å½•å¤±è´¥çš„åŒºå—
			s.recordFailedBlock(transfer.BlockNumber, err.Error())
			s.updateErrorCount()
			continue
		}
		successCount++
		// æ ‡è®°åŒºå—å·²å¤„ç†
		s.markBlockProcessed(transfer.BlockNumber)
	}

	// æ ‡è®°ä»»åŠ¡èŒƒå›´å†…çš„æ‰€æœ‰åŒºå—ä¸ºå·²å¤„ç†ï¼ˆå³ä½¿æ²¡æœ‰è½¬è´¦ï¼‰
	for blockNum := result.Task.StartBlock; blockNum <= result.Task.EndBlock; blockNum++ {
		s.markBlockProcessed(blockNum)
	}

	// æ›´æ–°åŒæ­¥çŠ¶æ€
	s.mu.Lock()
	s.status.SyncedCount += int64(successCount)
	s.status.LastSyncTime = time.Now()
	if result.Task.EndBlock > s.status.LastBlock {
		s.status.LastBlock = result.Task.EndBlock
	}
	s.mu.Unlock()

	// æ›´æ–°åŒæ­¥è¿›åº¦
	s.progressMu.Lock()
	s.syncProgress.CompletedTasks++
	s.progressMu.Unlock()

	// æ•°æ®åŒæ­¥æˆåŠŸåæ›´æ–°ç¼“å­˜
	if successCount > 0 {
		go s.updateCachesAfterSync()
	}

	// å®šæœŸè¿›è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
	if s.syncProgress.CompletedTasks%10 == 0 { // æ¯10ä¸ªä»»åŠ¡æ£€æŸ¥ä¸€æ¬¡
		go func() {
			if err := s.ensureDataConsistency(result.Task.StartBlock, result.Task.EndBlock); err != nil {
				utils.Error("Data consistency check failed for blocks %d-%d: %v",
					result.Task.StartBlock, result.Task.EndBlock, err)
			}
		}()
	}

	utils.Info("Processed blocks %d-%d: saved %d transfers",
		result.Task.StartBlock, result.Task.EndBlock, successCount)
}

// shouldRetryTask åˆ¤æ–­ä»»åŠ¡æ˜¯å¦åº”è¯¥é‡è¯•
func (s *SyncService) shouldRetryTask(task *SyncTask) bool {
	maxRetries := s.config.RetryConfig.MaxRetries
	if maxRetries == 0 {
		maxRetries = 3
	}

	// å¦‚æœé‡è¯•æ¬¡æ•°æœªè¾¾åˆ°ä¸Šé™ä¸”æœ‰å¤šä¸ªèŠ‚ç‚¹å¯ç”¨
	return task.RetryCount < maxRetries && len(s.config.TronNodes) > 1
}

// retryTaskWithDifferentNode ä½¿ç”¨ä¸åŒèŠ‚ç‚¹é‡è¯•ä»»åŠ¡
func (s *SyncService) retryTaskWithDifferentNode(task *SyncTask) {
	nodeCount := len(s.config.TronNodes)
	if nodeCount <= 1 {
		return // å•èŠ‚ç‚¹æ¨¡å¼æ— æ³•é‡åˆ†é…
	}

	// æ£€æŸ¥å¹¶å‘ç®¡ç†å™¨æ˜¯å¦å¯ç”¨
	if s.concurrentManager == nil || s.concurrentManager.taskChan == nil {
		utils.Warn("Concurrent manager not available, cannot reassign task for blocks %d-%d",
			task.StartBlock, task.EndBlock)
		// æ·»åŠ åˆ°å¤±è´¥é˜Ÿåˆ—
		for blockNum := task.StartBlock; blockNum <= task.EndBlock; blockNum++ {
			s.recordFailedBlock(blockNum, "concurrent manager unavailable")
		}
		return
	}

	// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
	select {
	case <-s.concurrentManager.ctx.Done():
		utils.Warn("Cannot reassign task, sync manager is stopping")
		// å¦‚æœæ— æ³•é‡åˆ†é…ï¼Œæ·»åŠ åˆ°å¤±è´¥é˜Ÿåˆ—
		for blockNum := task.StartBlock; blockNum <= task.EndBlock; blockNum++ {
			s.recordFailedBlock(blockNum, "task reassignment failed - manager stopping")
		}
		return
	default:
		// ç»§ç»­æ‰§è¡Œ
	}

	// é€‰æ‹©ä¸‹ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
	nextNodeIndex := (task.NodeIndex + 1) % nodeCount

	// åˆ›å»ºæ–°çš„é‡è¯•ä»»åŠ¡
	retryTask := &SyncTask{
		StartBlock: task.StartBlock,
		EndBlock:   task.EndBlock,
		NodeIndex:  nextNodeIndex,
		RetryCount: task.RetryCount + 1,
		CreatedAt:  time.Now(),
	}

	utils.Info("Retrying task for blocks %d-%d with node %d (attempt %d/%d)",
		retryTask.StartBlock, retryTask.EndBlock, nextNodeIndex,
		retryTask.RetryCount, s.config.RetryConfig.MaxRetries)

	// å°†é‡è¯•ä»»åŠ¡é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œä½¿ç”¨éé˜»å¡æ–¹å¼
	select {
	case s.concurrentManager.taskChan <- retryTask:
		utils.Info("Task reassigned to node %d for blocks %d-%d",
			nextNodeIndex, retryTask.StartBlock, retryTask.EndBlock)
	case <-s.concurrentManager.ctx.Done():
		utils.Warn("Cannot reassign task, sync manager is stopping")
		// å¦‚æœæ— æ³•é‡åˆ†é…ï¼Œæ·»åŠ åˆ°å¤±è´¥é˜Ÿåˆ—
		for blockNum := task.StartBlock; blockNum <= task.EndBlock; blockNum++ {
			s.recordFailedBlock(blockNum, "task reassignment failed - manager stopped")
		}
	default:
		utils.Warn("Task queue full, cannot reassign task for blocks %d-%d",
			task.StartBlock, task.EndBlock)
		// å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œæ·»åŠ åˆ°å¤±è´¥é˜Ÿåˆ—
		for blockNum := task.StartBlock; blockNum <= task.EndBlock; blockNum++ {
			s.recordFailedBlock(blockNum, "task reassignment failed - queue full")
		}
	}
}

// updateCachesAfterSync æ•°æ®åŒæ­¥åæ›´æ–°ç›¸å…³ç¼“å­˜
func (s *SyncService) updateCachesAfterSync() {
	utils.Debug("Updating caches after successful data sync")

	// æ›´æ–°ç»Ÿè®¡æ‘˜è¦ç¼“å­˜
	if s.transferService != nil {
		go func() {
			if _, err := s.transferService.GetStats(); err != nil {
				utils.Error("Failed to update stats cache after sync: %v", err)
			} else {
				utils.Debug("Stats cache updated successfully")
			}
		}()
	}

	// æ›´æ–°å¸‚åœºæ•°æ®ç¼“å­˜
	if s.marketService != nil {
		go func() {
			if _, err := s.marketService.GetDetailedUSDTData(); err != nil {
				utils.Error("Failed to update market cache after sync: %v", err)
			} else {
				utils.Debug("Market cache updated successfully")
			}
		}()
	}
}

// waitForAllTasksCompletion ç­‰å¾…æ‰€æœ‰å¹¶å‘ä»»åŠ¡å®Œæˆ
func (s *SyncService) waitForAllTasksCompletion(ctx context.Context, totalTasks int) error {
	utils.Info("Waiting for %d concurrent tasks to complete...", totalTasks)
	
	ticker := time.NewTicker(5 * time.Second) // æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡è¿›åº¦
	defer ticker.Stop()
	
	startTime := time.Now()
	timeout := 30 * time.Minute // 30åˆ†é’Ÿè¶…æ—¶
	
	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
			
		case <-ticker.C:
			// æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€
			s.progressMu.RLock()
			completedTasks := s.syncProgress.CompletedTasks
			s.progressMu.RUnlock()
			
			utils.Info("Concurrent sync progress: %d/%d tasks completed", completedTasks, totalTasks)
			
			// æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
			if completedTasks >= totalTasks {
				utils.Info("All concurrent sync tasks completed successfully in %v", time.Since(startTime))
				return nil
			}
			
			// æ£€æŸ¥è¶…æ—¶
			if time.Since(startTime) > timeout {
				return fmt.Errorf("concurrent sync timeout after %v, completed %d/%d tasks", timeout, completedTasks, totalTasks)
			}
			
			// æ£€æŸ¥å¹¶å‘ç®¡ç†å™¨çŠ¶æ€
			if s.concurrentManager != nil {
				select {
				case <-s.concurrentManager.ctx.Done():
					return fmt.Errorf("concurrent sync manager stopped unexpectedly")
				default:
					// ç»§ç»­ç­‰å¾…
				}
			}
		}
	}
}
